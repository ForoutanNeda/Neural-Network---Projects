{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_2_Task2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI4_W0iqq39Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install https://github.com/pytorch/text/archive/master.zip\n",
        "!pip install inscriptis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CWQx-cPs5rx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch    # root package\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tUO-irrs-2d",
        "colab_type": "code",
        "outputId": "d2624eb7-2af0-4797-8e01-269b785ed8bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\"\"\"\n",
        "Load the AG_NEWS dataset in bi-gram features format.\n",
        "\"\"\"\n",
        "# !pip install https://github.com/pytorch/text/archive/master.zip\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import text_classification\n",
        "import os\n",
        "\n",
        "NGRAMS = 2\n",
        "\n",
        "if not os.path.isdir('./.data'):\n",
        "    os.mkdir('./.data')\n",
        "\n",
        "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
        "    root='./.data', ngrams=NGRAMS, vocab=None)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120000lines [00:07, 16640.70lines/s]\n",
            "120000lines [00:14, 8162.13lines/s]\n",
            "7600lines [00:00, 8607.14lines/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJpvo3Ta86vL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# just to see the data\n",
        "print(len(train_dataset.get_labels()))\n",
        "print(test_dataset[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJCK81PXtECf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Import the necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "loss_name = \"BCELoss\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U5hs8NJtQ4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMmodel(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_dim, dim_embed, num_classes, dropout_gate = 0): \n",
        "    super(LSTMmodel, self).__init__()\n",
        "\n",
        "    self.dropout_gate = dropout_gate\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embeddings = nn.EmbeddingBag(vocab_size, dim_embed)\n",
        "\n",
        "    self.lstm = nn.LSTM(dim_embed, hidden_dim)\n",
        "    self.linear = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    # one hot encoded vectors\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    self.initialize_weights()\n",
        "\n",
        "  def initialize_weights(self):\n",
        "    num_range = 0.5\n",
        "    self.embeddings.weight.data.uniform_(-num_range, num_range)\n",
        "    self.linear.weight.data.uniform_(-num_range, num_range)\n",
        "    self.linear.bias.data.zero_()\n",
        "\n",
        "\n",
        "  def forward(self, x, offset):\n",
        "    embedding = self.embeddings(x,offset)\n",
        "    lstm_out, _ = self.lstm(embedding.view(len(embedding),1,-1))\n",
        "\n",
        "    if (self.dropout_gate):\n",
        "      lstm_out = self.dropout(lstm_out)\n",
        "    out = self.linear(lstm_out.view(len(lstm_out),-1))\n",
        "    return self.softmax(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr9_JuOoyDRc",
        "colab_type": "code",
        "outputId": "37b6ec76-5c7d-444e-c18b-a3c906fc7a15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
        "EMBED_DIM = 32\n",
        "HIDDEN_DIM = 16\n",
        "NUM_CLASS = len(train_dataset.get_labels())\n",
        "\n",
        "# making training model\n",
        "model = LSTMmodel(VOCAB_SIZE, HIDDEN_DIM, EMBED_DIM, NUM_CLASS, dropout_gate=0)\n",
        "model.cuda()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMmodel(\n",
              "  (embeddings): EmbeddingBag(1308844, 32, mode=mean)\n",
              "  (lstm): LSTM(32, 16)\n",
              "  (dropout): Dropout(p=0.85, inplace=False)\n",
              "  (linear): Linear(in_features=16, out_features=4, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zznYrd7Iyj76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(batch):\n",
        "    \n",
        "    label = torch.tensor([entry[0] for entry in batch])\n",
        "    text = [entry[1] for entry in batch]\n",
        "    offsets = [0] + [len(entry) for entry in text]\n",
        "\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text = torch.cat(text)\n",
        "    \n",
        "    return text, offsets, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ho5AyEY8G6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for one hot encoded vectors\n",
        "def convert_labels (x):\n",
        "  labels = np.zeros((x.size()[0], len(train_dataset.get_labels())))\n",
        "  for val in range(x.size()[0]):\n",
        "     labels[val][x[val]] = 1\n",
        "     \n",
        "  return torch.from_numpy(labels).type(torch.FloatTensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5U9nzDOzAG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_data, model):\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    # Initial values of training loss and training accuracy\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "\n",
        "    data = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
        "    \n",
        "    for i, (text, offsets, cls) in enumerate(data):\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "\n",
        "        # if criterion is BCELoss use one hot encoded vectors for labels\n",
        "        if (loss_name == \"BCELoss\"):\n",
        "          cls = convert_labels(cls).to(device)\n",
        "\n",
        "        output = model(text, offsets)\n",
        "        \n",
        "        loss = criterion(output, cls)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (loss_name == \"CrossEntropy\"):\n",
        "          train_acc += (output.argmax(1) == cls).sum().item()\n",
        "        else:\n",
        "          train_acc += (output.argmax(1) == cls.argmax(1)).sum().item()\n",
        "\n",
        "    scheduler.step()\n",
        "    \n",
        "    return train_loss / len(train_data), train_acc / len (train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDgn_vmszEB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(test_data, model):\n",
        "  \n",
        "    model.eval()\n",
        "    # Initial values of test loss and test accuracy\n",
        "    \n",
        "    loss = 0\n",
        "    acc = 0\n",
        "    \n",
        "    data = DataLoader(test_data, batch_size = BATCH_SIZE, collate_fn = generate_batch)\n",
        "    \n",
        "    for text, offsets, cls in data:\n",
        "\n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "        if (loss_name == \"BCELoss\"):\n",
        "          cls = convert_labels(cls).to(device)\n",
        "\n",
        "        # because during test we dont need to compute the grdient of any tensor\n",
        "        with torch.no_grad():\n",
        "          output = model(text, offsets)              \n",
        "\n",
        "          loss = criterion(output, cls)\n",
        "          loss += loss.item()\n",
        "          \n",
        "          if (loss_name == \"CrossEntropy\"):\n",
        "            acc += (output.argmax(1) == cls).sum().item()\n",
        "          else:\n",
        "            acc += (output.argmax(1) == cls.argmax(1)).sum().item()\n",
        "\n",
        "    return loss / len(test_data), acc / len(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhOli65ICQLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mount drive (needed for my own training)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWZhC_wvJutZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading pre-trained weights to the model\n",
        "# given path for pretrained model (for fine tuning, DONT RUN IF YOU DO NOT WANT TO FINE TUNE)\n",
        "model.load_state_dict(torch.load('../weights/task2_saved_weights_test.pt'))\n",
        "# In case you want to freeze the layers\n",
        "for name, param in model.named_parameters():\n",
        "  if \"embedding\" in name:\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks2TdzRDMgXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print parameters of the model and the name\n",
        "for name, param in model.named_parameters():\n",
        "  print (name, param.requires_grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRawXcK95cH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import torchvision\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "# Hyper parametre selection\n",
        "\n",
        "N_EPOCHS = 15\n",
        "LEARNING_RATE = 1e-4\n",
        "TRAIN_RATIO = 0.9\n",
        "\n",
        "valid_loss = float('inf')\n",
        "\n",
        "# Use the appropriate loss function\n",
        "if loss_name == \"BCELoss\":\n",
        "  criterion = torch.nn.BCELoss().to(device)\n",
        "else:\n",
        "  criterion = torch.nn.CrossEntropyLoss().to(device) \n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= LEARNING_RATE, weight_decay=1e-5)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 4, gamma=0.1)\n",
        "valid_acc_dummy = 1e-3\n",
        "\n",
        "# TODO: Split the data into train and validation sets using random_split()\n",
        "train_len = int(len(train_dataset) * TRAIN_RATIO)\n",
        "valid_len = len(train_dataset) - train_len\n",
        "train_split_dataset , valid_split_dataset = random_split(train_dataset, [train_len, valid_len])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QdRFl_HzHSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training Code\n",
        "\n",
        "# logs for graphs\n",
        "valid_loss_array = []\n",
        "train_loss_array = []\n",
        "train_acc_array = []\n",
        "valid_acc_array = []\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train(train_split_dataset, model)\n",
        "    # Log of accuracy and error\n",
        "    train_loss_array.append(train_loss)\n",
        "    train_acc_array.append(train_acc)\n",
        "\n",
        "    # test on validation set to compute the error and accuracy on validation set\n",
        "    valid_loss, valid_acc = test(valid_split_dataset, model)\n",
        "    \n",
        "    valid_loss_array.append(valid_loss)\n",
        "    valid_acc_array.append(valid_acc)\n",
        "    # save model of the best accuracy \n",
        "    if (valid_acc > valid_acc_dummy):\n",
        "      # for your own path change the path\n",
        "      # torch.save(model.state_dict(), '/content/gdrive/My Drive/weights_NN/Experiment2/'+ 'final_fine_' + str(valid_acc) + \"_.pt\") \n",
        "      valid_acc_dummy = valid_acc\n",
        "\n",
        "    secs = int(time.time() - start_time)\n",
        "    mins = secs / 60\n",
        "    secs = secs % 60\n",
        "\n",
        "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5X-B_KprlGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to normalize accuracies and error for visualizations of trend\n",
        "def normalize_logs(arr):\n",
        "  a = [x - min(arr) for x in arr]\n",
        "  val = max(arr) - min(arr)\n",
        "  a = [x/val for x in a]\n",
        "  return a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfKfETC3sN0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_logs(valid_array, train_array, epochs, label_log=\"loss\"):\n",
        "  import matplotlib.pyplot as plt\n",
        "  plt.figure(figsize=(5,5))\n",
        "  axes = plt.gca()\n",
        "  # line 1 points \n",
        "  x1 = list(range(0, N_EPOCHS)) \n",
        "  y1 = normalize_logs(valid_array)\n",
        "  # plotting the line 1 points  \n",
        "  plt.plot(x1, y1, ls='--', marker='o', color='b', label = \"valid_\" + label_log) \n",
        "    \n",
        "  # line 2 points \n",
        "  x2 = list(range(0, N_EPOCHS))\n",
        "  y2 = normalize_logs(train_array)\n",
        "  # plotting the line 2 points  \n",
        "  plt.plot(x2, y2, ls='--', marker='o', color='r', label = \"train_\"+ label_log) \n",
        "    \n",
        "  # naming the x axis \n",
        "  plt.xlabel('epochs')\n",
        "  axes.set_ylim([0,2])\n",
        "\n",
        "  # naming the y axis \n",
        "  plt.ylabel(\"normalized \" + label_log + 's') \n",
        "  # giving a title to my graph \n",
        "  plt.title(label_log + ' trend of train and validation!') \n",
        "\n",
        "  # show a legend on the plot \n",
        "  plt.legend()\n",
        "    \n",
        "  # function to show the plot\n",
        "  # saving in folder\n",
        "  # plt.savefig(\"/content/gdrive/My Drive/weights_NN/Graphs/task2_BCELoss\" + label_log + \".pdf\")\n",
        "  plt.show() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6umr4ls5q3vv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_logs(valid_loss_array, train_loss_array , N_EPOCHS, label_log=\"loss\") # plot loss\n",
        "plot_logs(valid_acc_array, train_acc_array , N_EPOCHS, label_log=\"accuracy\") # plot accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2Lj-R1LzKy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Building test model and loading the saved model...')\n",
        "\n",
        "VOCAB_SIZE_TEST = len(test_dataset.get_vocab())\n",
        "EMBED_DIM_TEST = 32\n",
        "HIDDEN_DIM_TEST = 16\n",
        "NUM_CLASS_TEST = len(test_dataset.get_labels())\n",
        "\n",
        "model_test = LSTMmodel(VOCAB_SIZE_TEST,HIDDEN_DIM_TEST,EMBED_DIM_TEST,NUM_CLASS_TEST)\n",
        "# load trained model \n",
        "# path given according to the directory\n",
        "# Change directory according to your system to make it work\n",
        "model_test.load_state_dict(torch.load('../weights/task2_saved_weights_test.pt'))\n",
        "model_test.cuda().eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjBF-qdC4wml",
        "colab_type": "code",
        "outputId": "a904341a-8bea-426d-8e6e-f9f7c25cfbe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing error calculation\n",
        "print(\"Computing testing Accuracy\")\n",
        "test_loss, test_acc = test(test_dataset, model_test)\n",
        "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tLoss: 0.0001(test)\t|\tAcc: 92.0%(test)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCMb0Z5NlmQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# used for html extraction\n",
        "import urllib.request\n",
        "from inscriptis import get_text\n",
        "\n",
        "def get_text_url(url_link):\n",
        "  html = urllib.request.urlopen(url_link).read().decode('utf-8')\n",
        "  return get_text(html)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkDlZzsKO5aX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing necessary libraries\n",
        "\n",
        "import re\n",
        "from torchtext.data.utils import ngrams_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# labels for the AG_NEWS dataset\n",
        "\n",
        "ag_news_label = {1 : \"World\",\n",
        "                 2 : \"Sports\",\n",
        "                 3 : \"Business\",\n",
        "                 4 : \"Sci/Tec\"}\n",
        "\n",
        "def predict(text, model, vocab, ngrams):\n",
        "    tokenizer = get_tokenizer(\"basic_english\")\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor([vocab[token]\n",
        "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
        "        output = model(text, torch.tensor([0]))\n",
        "        return output.argmax(1).item() + 1\n",
        "\n",
        "vocab = train_dataset.get_vocab()\n",
        "# change this to model if you want to use the trained model otherwise we use pre trained model loaded into test\n",
        "model_test = model_test.to(\"cpu\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9hwXYpO4sSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TESTING WITH RANDOM TEXTS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67I4VdPqOmMx",
        "colab_type": "code",
        "outputId": "c7cf64df-caad-48fe-8a81-5534a1869d60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# testing using urlib for text extraction\n",
        "\n",
        "url_random_travel = \"https://www.nytimes.com/2020/01/22/sports/football/eli-manning-retirement.html\"\n",
        "# chand model_test to model if you want to use trained model\n",
        "print(\"This is a '%s' news\" % ag_news_label[predict(get_text_url(url_random_travel), model_test, vocab, NGRAMS)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a 'Sports' news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHX-8VRhPDRk",
        "colab_type": "code",
        "outputId": "9f4e3eb3-8e84-4a5b-ffd1-f29b9fd3d639",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text_test = \"Amazon, Alphabet, Alibaba, Facebook, Tencent - five of the world's 10 most valuable companies,\\\n",
        "all less than 25 years old - and all got rich, in their own ways, on data.No wonder it's become common to call data the 'new oil'.\\\n",
        "As recently as 2011, five of the top 10 were oil companies. Now, only ExxonMobil clings on.\\\n",
        "The analogy isn't perfect. Data can be used many times, oil only once.\\\n",
        "But data is like oil in that the crude, unrefined stuff is not much use to anyone.\\\n",
        "You have to process it to get something valuable. fortnightYou refine oil to make diesel, to put it in an engine.\\\n",
        "With data, you need to analyse it to provide insights that can inform decisions - which advert to insert in a social media timeline, which search result to put at the top of the page.\\\n",
        "Imagine you were asked to make just one of those decisions.\\\n",
        "Someone is watching a video on YouTube, which is run by Google, which is owned by Alphabet. What should the system suggest they watch next?\\\n",
        " Pique their interest, and YouTube gets to serve them another advert. Lose their attention, and they will click away.\\\n",
        "You have all the data you need. Consider every other YouTube video they have ever watched - what are they interested in? Now, look at what other users have gone on to watch after this video.\\\n",
        "Weigh up the options, calculate probabilities. If you choose wisely, and they view another ad, well done - you've earned Alphabet all of, ooh, maybe 20 cents (15p).\\\n",
        "Clearly, relying on humans to process data would be impossibly inefficient. These business models need machines.\\\n",
        "In the data economy, power comes not from data alone but from the interplay of data and algorithm.\"\n",
        "\n",
        "print(\"This is a '%s' news\" % ag_news_label[predict(text_test, model, vocab, NGRAMS)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a 'Business' news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmlzCaiDQ7E6",
        "colab_type": "code",
        "outputId": "343c1b02-c23d-4756-e6ec-ade25e80618b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_text2 = \"Demand for iPhones appears to be flourishing once again in China, a year after Apple had to warn investors that the Chinese market was facing a serious slow down.\\\n",
        "IPhone sales in China were up 18% in December from the same month a year earlier, an even better performance than Wall Street had projected, according to an investor note from Wedbush analyst Dan Ives. Apple (AAPL) shipped around 3.2 million iPhones to China during the month compared to 2.7 million in December 2018, Ives reported, citing data from the China Academy of Information and Communication Technology.\\\n",
        "It's good news for Apple, after iPhone sales tumbled in China over the past year.\\\n",
        "'Our belief that China will continue this positive upward trajectory with renewed growth and share gains on the heels of an iPhone 11 product cycle which the skeptics continue to underestimate,' Ives said in the Thursday note.\\\n",
        "The good news was reflected in Apple's stock, which was up nearly 2% to a record high on Thursday.\\\n",
        "China is a key market for Apple — the region makes up nearly 17% of the company's total sales. And the iPhone is Apple's biggest profit driver.\\\n",
        "In early January 2019, Apple CEO Tim Cook wrote a letter to investors warning them to expect lower sales from the holiday quarter due primarily to iPhone sales in China falling short of what the company had expected. It was the first time since June 2002 that Apple issued a reduction in its quarterly revenue forecast. When the company reported earnings for that quarter later in January, iPhone sales had fallen 15% from the prior year.\\\n",
        "A number of factors contributed to the drop, namely slower growth in the Chinese economy and the US-China trade war.\\\n",
        "The trend continued throughout much of last year.\\\n",
        "In April, Apple said its iPhone sales in the first three months of 2019 dropped 17% from the same period a year earlier, again because of sluggish demand in China. A month later, Citi analysts warned that the trade war could cause Apple's iPhone sales in China to be cut in half.\\\n",
        "In the three months ending in June 2019, iPhones made up less than half of the company's revenue for the first time in years, though the slump also coincided with a greater focus at Apple on subscription-based services such as Apple Music.\\\n",
        "But the iPhone 11, which Apple introduced in September with better camera technology and battery life, as well as lower-than-expected prices, has helped with the rebound, Ives said. Early demand for the new model was strong, and in Apple's October earnings call, Cook noted that the company's prospects in China were turning around.\\\n",
        "Now, Ives estimates that there are roughly 60 million to 70 million iPhone users in China who are likely to upgrade their phones in the coming months.\\\n",
        "The momentum probably will continue this year, as Apple analysts widely expect Apple to release a 5G-enabled version of the iPhone in the fall.\\\n",
        "'Many investors are asking us: Is all the good news baked into shares after an historic upward move over the last year?' Ives said in the note. 'The answer from our vantage point is a resounding NO, as we view [this as] only the first part of this massive upgrade opportunity.\"\n",
        "\n",
        "print(\"This is a '%s' news\" % ag_news_label[predict(test_text2, model, vocab, NGRAMS)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a 'Sci/Tec' news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5LVAz9YRaVP",
        "colab_type": "code",
        "outputId": "bee391ae-6e99-40db-ec00-d91a8908530c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "fortnight = \"Microsoft on Wednesday said it conducted an investigation into a security breach of one of its customer databases and found records could have been exposed for\\\n",
        "a short period in December. A misconfiguration in a database's Azure security rules on Dec. 5 enabled exposure to millions of customer support records, according to a blog post from Microsoft on Wednesday. After being alerted of the issue, \\\n",
        "engineers fixed the problem as of Dec. 31. The company says there was no malicious use of the data but is disclosing the breach to be transparent to its customers.\\\n",
        "Misconfigurations are unfortunately a common error across the industry, the company said.\\\n",
        "We have solutions to help prevent this kind of mistake, but unfortunately, they were not enabled for this database. As we've learned, it is good to periodically review your own configurations and ensure you are taking advantage of all protections available.\\\n",
        "Most customer data stored in the databases had personal information redacted, Microsoft said. The company said it'll contact customers whose info may have not been redacted.\\\n",
        "Bob Diachenko, a security researcher with Comparitech, discovered the security lapse on Dec. 28. He alerted Microsoft about the issue on Dec. 29 leading to the fix two days later. \"\n",
        "\n",
        "print(\"This is a '%s' news\" % ag_news_label[predict(fortnight, model, vocab, NGRAMS)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a 'Sci/Tec' news\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
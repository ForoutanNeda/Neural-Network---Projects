{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_2_Task3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jed_sca5uass",
        "colab_type": "text"
      },
      "source": [
        "# **WE DID NOT ADD THE WEIGHT OF THE TASK3 TO WEIGHT FOLDER IN ZIP FILE DUE TO SIZE CONSTRAINTS. IF YOU NEED IT YOU CAN ASK FOR IT, OTHERWISE TRAIN THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI4_W0iqq39Y",
        "colab_type": "code",
        "outputId": "e0852af2-ef38-487a-8547-72e5f4f9ea32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        }
      },
      "source": [
        "!pip install https://github.com/pytorch/text/archive/master.zip\n",
        "!pip install inscriptis"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/pytorch/text/archive/master.zip\n",
            "  Using cached https://github.com/pytorch/text/archive/master.zip\n",
            "Requirement already satisfied (use --upgrade to upgrade): torchtext==0.5.1 from https://github.com/pytorch/text/archive/master.zip in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5.1) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5.1) (2.21.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5.1) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5.1) (1.17.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5.1) (1.12.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5.1) (0.1.85)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.5.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.5.1) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.5.1) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.5.1) (3.0.4)\n",
            "Building wheels for collected packages: torchtext\n",
            "  Building wheel for torchtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchtext: filename=torchtext-0.5.1-cp36-none-any.whl size=64241 sha256=9b9130bf16ac4936a3b54274e6950fea0c1d82c7e3beeddfdaa94bc51e0df2de\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-46u8m37b/wheels/5a/86/3d/30ae7dfdfeb1748bb11b3da173fb9634141fbb39e9e9847317\n",
            "Successfully built torchtext\n",
            "Requirement already satisfied: inscriptis in /usr/local/lib/python3.6/dist-packages (1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from inscriptis) (2.21.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from inscriptis) (4.2.6)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->inscriptis) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->inscriptis) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->inscriptis) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->inscriptis) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CWQx-cPs5rx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch    # root package\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tUO-irrs-2d",
        "colab_type": "code",
        "outputId": "2738e481-077c-416e-b6f2-1e2f20e9d075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "\"\"\"\n",
        "Load the AG_NEWS dataset in bi-gram features format.\n",
        "\"\"\"\n",
        "# !pip install https://github.com/pytorch/text/archive/master.zip\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import text_classification\n",
        "import os\n",
        "\n",
        "NGRAMS = 2\n",
        "\n",
        "if not os.path.isdir('./.data'):\n",
        "    os.mkdir('./.data')\n",
        "\n",
        "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
        "    root='./.data', ngrams=NGRAMS, vocab=None)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120000lines [00:07, 16533.90lines/s]\n",
            "120000lines [00:14, 8120.77lines/s]\n",
            "7600lines [00:00, 8682.63lines/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJCK81PXtECf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Import the necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "loss_name = \"BCELoss\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82JxbtfOk4sq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMmodel(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_dim, dim_embed, num_classes, dropout_gate = 0): \n",
        "    super(LSTMmodel, self).__init__()\n",
        "\n",
        "    self.dropout_gate = dropout_gate\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embeddings = nn.EmbeddingBag(vocab_size, dim_embed)\n",
        "\n",
        "    # convelution with kernel of [1,5], the in_channels = 1, the out_channels = 1\n",
        "    self.conv =nn.Conv2d(1, 1, [1,5])  \n",
        "\n",
        "    # After Convolution the input_size = (dim_embed - 5 + 1)\n",
        "    self.lstm = nn.LSTM((dim_embed-4), hidden_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(p=0.85)\n",
        "    self.linear = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    # one hot encoded vectors\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    self.initialize_weights()\n",
        "\n",
        "  def initialize_weights(self):\n",
        "    num_range = 0.5\n",
        "    self.embeddings.weight.data.uniform_(-num_range, num_range)\n",
        "    self.linear.weight.data.uniform_(-num_range, num_range)\n",
        "    self.linear.bias.data.zero_()\n",
        "\n",
        "\n",
        "  def forward(self, x, offset):\n",
        "    embedding = self.embeddings(x,offset)   #[Batch_size (B), Dim_embed (D)]\n",
        "    sq_embed = embedding.unsqueeze(0).unsqueeze(1)    #[1, 1, B, D]\n",
        "    conv_out = self.conv(sq_embed)    #[1, 1, B, D-5+1]\n",
        "    relu_conv = F.relu(conv_out).squeeze()  #[B, D-5+1]\n",
        "    lstm_out, _ = self.lstm(relu_conv.view(len(relu_conv),1,-1)) \n",
        "    if (self.dropout_gate):\n",
        "      lstm_out = self.dropout(lstm_out)\n",
        "    out = self.linear(lstm_out.view(len(lstm_out),-1))\n",
        "    return self.softmax(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr9_JuOoyDRc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "d82267a1-dbd5-43b0-93a2-89e5a3554de4"
      },
      "source": [
        "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
        "EMBED_DIM = 32    #dimension of embedding\n",
        "HIDDEN_DIM = 16   #The number of features in the hidden state\n",
        "NUM_CLASS = len(train_dataset.get_labels())  #number of classes\n",
        "\n",
        "# making training model\n",
        "model = LSTMmodel(VOCAB_SIZE, HIDDEN_DIM, EMBED_DIM, NUM_CLASS, dropout_gate=0)\n",
        "model.cuda()"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMmodel(\n",
              "  (embeddings): EmbeddingBag(1308844, 32, mode=mean)\n",
              "  (conv): Conv2d(1, 1, kernel_size=[1, 5], stride=(1, 1))\n",
              "  (lstm): LSTM(28, 16)\n",
              "  (dropout): Dropout(p=0.85, inplace=False)\n",
              "  (linear): Linear(in_features=16, out_features=4, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4rpreD7imDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(batch):\n",
        "    \n",
        "    label = torch.tensor([entry[0] for entry in batch])\n",
        "    text = [entry[1] for entry in batch]\n",
        "    offsets = [0] + [len(entry) for entry in text]\n",
        "\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text = torch.cat(text)\n",
        "    \n",
        "    return text, offsets, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ho5AyEY8G6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for one hot encoded vectors\n",
        "def convert_labels (x):\n",
        "  labels = np.zeros((x.size()[0], len(train_dataset.get_labels())))\n",
        "  for val in range(x.size()[0]):\n",
        "     labels[val][x[val]] = 1\n",
        "     \n",
        "  return torch.from_numpy(labels).type(torch.FloatTensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5U9nzDOzAG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_data, model):\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    # Initial values of training loss and training accuracy\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "\n",
        "    data = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
        "    \n",
        "    for i, (text, offsets, cls) in enumerate(data):\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "\n",
        "        # if criterion is BCELoss use one hot encoded vectors for labels\n",
        "        if (loss_name == \"BCELoss\"):\n",
        "          cls = convert_labels(cls).to(device)\n",
        "\n",
        "        output = model(text, offsets)\n",
        "        \n",
        "        loss = criterion(output, cls)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (loss_name == \"CrossEntropy\"):\n",
        "          train_acc += (output.argmax(1) == cls).sum().item()\n",
        "        else:\n",
        "          train_acc += (output.argmax(1) == cls.argmax(1)).sum().item()\n",
        "\n",
        "    scheduler.step()\n",
        "    \n",
        "    return train_loss / len(train_data), train_acc / len (train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDgn_vmszEB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(test_data, model):\n",
        "  \n",
        "    model.eval()\n",
        "    # Initial values of test loss and test accuracy\n",
        "    \n",
        "    loss = 0\n",
        "    acc = 0\n",
        "    \n",
        "    data = DataLoader(test_data, batch_size = BATCH_SIZE, collate_fn = generate_batch)\n",
        "    \n",
        "    for text, offsets, cls in data:\n",
        "\n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "        if (loss_name == \"BCELoss\"):\n",
        "          cls = convert_labels(cls).to(device)\n",
        "\n",
        "        # because during test we dont need to compute the grdient of any tensor\n",
        "        with torch.no_grad():\n",
        "          output = model(text, offsets)              \n",
        "\n",
        "          loss = criterion(output, cls)\n",
        "          loss += loss.item()\n",
        "          \n",
        "          if (loss_name == \"CrossEntropy\"):\n",
        "            acc += (output.argmax(1) == cls).sum().item()\n",
        "          else:\n",
        "            acc += (output.argmax(1) == cls.argmax(1)).sum().item()\n",
        "\n",
        "    return loss / len(test_data), acc / len(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mRyEp2KnPRae",
        "outputId": "3f6b8fac-4a79-4572-d4cc-e9edc604156b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# mount drive (needed for my own training)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uV7nwJPsPQkU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "54306189-4965-43d9-e8ea-6282ff748093"
      },
      "source": [
        "# loading pre-trained weights to the model\n",
        "# given path for pretrained model (for fine tuning, DONT RUN IF YOU DO NOT WANT TO FINE TUNE)\n",
        "model.load_state_dict(torch.load('/content/gdrive/My Drive/weights_NN/finetune2_0.9325_.pt'))\n",
        "# for name, param in model.named_parameters():\n",
        "#   if \"embedding\" in name:\n",
        "#     param.requires_grad = False"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks2TdzRDMgXo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "429fec66-7cd8-4bb0-9eb3-bbfce12cba25"
      },
      "source": [
        "# Print parameters of the model and the name\n",
        "for name, param in model.named_parameters():\n",
        "  print (name, param.requires_grad)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embeddings.weight True\n",
            "conv.weight True\n",
            "conv.bias True\n",
            "lstm.weight_ih_l0 True\n",
            "lstm.weight_hh_l0 True\n",
            "lstm.bias_ih_l0 True\n",
            "lstm.bias_hh_l0 True\n",
            "linear.weight True\n",
            "linear.bias True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRawXcK95cH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import torchvision\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "# Hyper parametre selection\n",
        "\n",
        "N_EPOCHS = 15\n",
        "LEARNING_RATE = 1e-2\n",
        "TRAIN_RATIO = 0.9\n",
        "\n",
        "valid_loss = float('inf')\n",
        "\n",
        "# Use the appropriate loss function\n",
        "if loss_name == \"BCELoss\":\n",
        "  criterion = torch.nn.BCELoss().to(device)\n",
        "else:\n",
        "  criterion = torch.nn.CrossEntropyLoss().to(device) \n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= LEARNING_RATE, weight_decay=1e-5)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.6)\n",
        "valid_acc_dummy = 1e-3\n",
        "\n",
        "# TODO: Split the data into train and validation sets using random_split()\n",
        "train_len = int(len(train_dataset) * TRAIN_RATIO)\n",
        "valid_len = len(train_dataset) - train_len\n",
        "train_split_dataset , valid_split_dataset = random_split(train_dataset, [train_len, valid_len])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QdRFl_HzHSC",
        "colab_type": "code",
        "outputId": "ed2ebaf6-a2e9-4ea4-a790-435999b9dbbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        }
      },
      "source": [
        "# Training Code\n",
        "\n",
        "# logs for graphs\n",
        "valid_loss_array = []\n",
        "train_loss_array = []\n",
        "train_acc_array = []\n",
        "valid_acc_array = []\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train(train_split_dataset, model)\n",
        "    # Log of accuracy and error\n",
        "    train_loss_array.append(train_loss)\n",
        "    train_acc_array.append(train_acc)\n",
        "\n",
        "    # test on validation set to compute the error and accuracy on validation set\n",
        "    valid_loss, valid_acc = test(valid_split_dataset, model)\n",
        "    \n",
        "    valid_loss_array.append(valid_loss)\n",
        "    valid_acc_array.append(valid_acc)\n",
        "    # save model of the best accuracy\n",
        "    if (valid_acc > valid_acc_dummy):\n",
        "      # for your own path change the path\n",
        "      #torch.save(model.state_dict(), '/content/gdrive/My Drive/weights_NN/'+ 'CNN_LSTM2_' + str(valid_acc) + \"_.pt\")\n",
        "      valid_acc_dummy = valid_acc\n",
        "\n",
        "    secs = int(time.time() - start_time)\n",
        "    mins = secs / 60\n",
        "    secs = secs % 60\n",
        "\n",
        "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  | time in 2 minutes, 3 seconds\n",
            "\tLoss: 0.0050(train)\t|\tAcc: 87.3%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 90.1%(valid)\n",
            "Epoch: 2  | time in 2 minutes, 3 seconds\n",
            "\tLoss: 0.0034(train)\t|\tAcc: 92.0%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 91.4%(valid)\n",
            "Epoch: 3  | time in 2 minutes, 3 seconds\n",
            "\tLoss: 0.0026(train)\t|\tAcc: 94.0%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 92.1%(valid)\n",
            "Epoch: 4  | time in 2 minutes, 3 seconds\n",
            "\tLoss: 0.0018(train)\t|\tAcc: 95.8%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 92.5%(valid)\n",
            "Epoch: 5  | time in 2 minutes, 3 seconds\n",
            "\tLoss: 0.0012(train)\t|\tAcc: 97.5%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 92.7%(valid)\n",
            "Epoch: 6  | time in 2 minutes, 2 seconds\n",
            "\tLoss: 0.0007(train)\t|\tAcc: 98.6%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 92.3%(valid)\n",
            "Epoch: 7  | time in 2 minutes, 2 seconds\n",
            "\tLoss: 0.0004(train)\t|\tAcc: 99.2%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 92.3%(valid)\n",
            "Epoch: 8  | time in 2 minutes, 3 seconds\n",
            "\tLoss: 0.0003(train)\t|\tAcc: 99.5%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 92.1%(valid)\n",
            "Epoch: 9  | time in 2 minutes, 2 seconds\n",
            "\tLoss: 0.0002(train)\t|\tAcc: 99.7%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 92.1%(valid)\n",
            "Epoch: 10  | time in 2 minutes, 2 seconds\n",
            "\tLoss: 0.0001(train)\t|\tAcc: 99.8%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 92.0%(valid)\n",
            "Epoch: 11  | time in 2 minutes, 2 seconds\n",
            "\tLoss: 0.0001(train)\t|\tAcc: 99.9%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 91.9%(valid)\n",
            "Epoch: 12  | time in 2 minutes, 3 seconds\n",
            "\tLoss: 0.0001(train)\t|\tAcc: 99.9%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 91.9%(valid)\n",
            "Epoch: 13  | time in 2 minutes, 3 seconds\n",
            "\tLoss: 0.0001(train)\t|\tAcc: 99.9%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 91.9%(valid)\n",
            "Epoch: 14  | time in 2 minutes, 3 seconds\n",
            "\tLoss: 0.0001(train)\t|\tAcc: 99.9%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 92.0%(valid)\n",
            "Epoch: 15  | time in 2 minutes, 2 seconds\n",
            "\tLoss: 0.0001(train)\t|\tAcc: 99.9%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 91.9%(valid)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dMw1EQf5Kbj",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2Lj-R1LzKy4",
        "colab_type": "code",
        "outputId": "0dccf721-8db4-4829-8b67-8e1249fb55df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "print('Building test model and loading the saved model...')\n",
        "\n",
        "VOCAB_SIZE_TEST = len(test_dataset.get_vocab())\n",
        "EMBED_DIM_TEST = 32\n",
        "HIDDEN_DIM_TEST = 16\n",
        "NUM_CLASS_TEST = len(test_dataset.get_labels())\n",
        "\n",
        "model_test = LSTMmodel(VOCAB_SIZE_TEST,HIDDEN_DIM_TEST,EMBED_DIM_TEST,NUM_CLASS_TEST)\n",
        "# load trained model \n",
        "# path given according to the directory\n",
        "# Change directory according to your system to make it work\n",
        "model_test.load_state_dict(torch.load('/content/gdrive/My Drive/weights_NN/iter2_0.9329166666666666_.pt'))\n",
        "model_test.cuda().eval()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building test model and loading the saved model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMmodel(\n",
              "  (embeddings): EmbeddingBag(1308844, 32, mode=mean)\n",
              "  (conv): Conv2d(1, 1, kernel_size=[1, 5], stride=(1, 1))\n",
              "  (lstm): LSTM(28, 16)\n",
              "  (dropout): Dropout(p=0.85, inplace=False)\n",
              "  (linear): Linear(in_features=16, out_features=4, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjBF-qdC4wml",
        "colab_type": "code",
        "outputId": "fc9bad76-4baa-4618-d76a-bb6e3996f088",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Testing error calculation\n",
        "\n",
        "test_loss, test_acc = test(test_dataset, model_test)\n",
        "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tLoss: 0.0001(test)\t|\tAcc: 92.3%(test)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
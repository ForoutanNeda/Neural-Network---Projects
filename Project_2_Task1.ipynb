{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Project_2_Task1.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XaBVaOycBLF",
        "colab_type": "code",
        "outputId": "592a9779-9d60-44b2-a017-2f3c33fbe192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "!pip install torchtext==0.4"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n",
            "\r\u001b[K     |██████▏                         | 10kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 20kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 30kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 40kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 51kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (2.21.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.17.5)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (2.8)\n",
            "Installing collected packages: torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed torchtext-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sV39ML99Ydp",
        "colab_type": "text"
      },
      "source": [
        "# Project 3: Text Classification in PyTorch\n",
        "\n",
        "## Instructions\n",
        "\n",
        "* All the tasks that you need to complete in this project are either coding tasks (mentioned inside the code cells of the notebook with `#TODO` notations) or theoretical questions that you need to answer by editing the markdown question cells.\n",
        "* **Please make sure you read the [Notes](#Important-Notes) section carefully before you start the project.**\n",
        "\n",
        "## Introduction\n",
        "This project deals with neural text classification using PyTorch. Text classification is the process of assigning tags or categories to text according to its content. It's one of the fundamental tasks in Natural Language Processing (NLP) with broad applications such as sentiment analysis, topic labeling, spam detection, and intent detection.\n",
        "\n",
        "Text classification algorithms are at the heart of a variety of software systems that process text data at scale. Email software uses text classification to determine whether incoming mail is sent to the inbox or filtered into the spam folder. Discussion forums use text classification to determine whether comments should be flagged as inappropriate.\n",
        "\n",
        "**_Example:_** A simple example of text classification would be Spam Classification. Consider the bunch of emails that you would receive in the your personal inbox if the email service provider did not have a spam filter algorithm. Because of the spam filter, spam emails get redirected to the Spam folder, while you receive only non-spam (\"_ham_\") emails in your inbox.\n",
        "\n",
        "![](http://blog.yhat.com/static/img/spam-filter.png)\n",
        "\n",
        "## Task\n",
        "Here, we want you to focus on a specific type of text classification task, \"Document Classification into Topics\". It can be addressed as classifying text data or even large documents into separate discrete topics/genres of interest.\n",
        "\n",
        "\n",
        "![](https://miro.medium.com/max/700/1*YWEqFeKKKzDiNWy5UfrTsg.png)\n",
        "\n",
        "In this project, you will be working on classifying given text data into discrete topics or genres. You are given a bunch of text data, each of which has a label attached. We ask you to learn why you think the contents of the documents have been given these labels based on their words. You need to create a neural classifier that is trained on this given information. Once you have a trained classifier, it should be able to predict the label for any new document or text data sample that is fed to it. The labels need not have any meaning to us, nor to you necessarily.\n",
        "\n",
        "## Data\n",
        "There are various datasets that we can use for this purpose. This tutorial shows how to use the text classification datasets in the PyTorch library ``torchtext``. There are different datasets in this library like `AG_NEWS`, `SogouNews`, `DBpedia`, and others. This project will deal with training a supervised learning algorithm for classification using one of these datasets. In task 1 of this project, we will work with the `AG_NEWS` dataset.\n",
        "\n",
        "## Load Data\n",
        "\n",
        "A bag of **ngrams** feature is applied to capture some partial information about the local word order. In practice, bi-grams or tri-grams are applied to provide more benefits as word groups than only one word.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "*\"I love Neural Networks\"*\n",
        "* **Bi-grams:** \"I love\", \"love Neural\", \"Neural Networks\"\n",
        "* **Tri-grams:** \"I love Neural\", \"love Neural Networks\"\n",
        "\n",
        "In the code below, we have loaded the `AG_NEWS` dataset from the ``torchtext.datasets.TextClassification`` package with bi-grams feature. The dataset supports the ngrams method. By setting ngrams to 2, the example text in the dataset will be a list of single words plus bi-grams string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAxiNjqd9Yds",
        "colab_type": "code",
        "outputId": "cb84ccf7-4480-40e0-b3f8-b5a1bc72dddd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "\"\"\"\n",
        "Load the AG_NEWS dataset in bi-gram features format.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import text_classification\n",
        "import os\n",
        "\n",
        "NGRAMS = 2\n",
        "\n",
        "if not os.path.isdir('./.data'):\n",
        "    os.mkdir('./.data')\n",
        "\n",
        "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
        "    root='./.data', ngrams=NGRAMS, vocab=None)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120000lines [00:06, 17539.50lines/s]\n",
            "120000lines [00:14, 8318.45lines/s]\n",
            "7600lines [00:00, 8879.91lines/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNwoldVg9Ydv",
        "colab_type": "text"
      },
      "source": [
        "## Model\n",
        "\n",
        "Our first simple model is composed of an [`EmbeddingBag`](https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag) layer and a linear layer.\n",
        "\n",
        "``EmbeddingBag`` computes the mean value of a “bag” of embeddings. The text entries here have different lengths. ``EmbeddingBag`` requires no padding here since the text lengths are saved in offsets. Additionally, since ``EmbeddingBag`` accumulates the average across the embeddings on the fly, ``EmbeddingBag`` can enhance the performance and memory efficiency to process a sequence of tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n39Wgjon9Ydw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Import the necessary libraries\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# TODO: Create a class TextClassifier. Remember that this class will be your model.\n",
        "class text_classifier(nn.Module):\n",
        "    # TODO: Define the __init__() method with proper parameters\n",
        "    # (vocabulary size, dimensions of the embeddings, number of classes)\n",
        "    def __init__(self, vocab_size, dim_embed, num_class):\n",
        "        super().__init__()\n",
        "        # TODO: define the embedding layer\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, dim_embed, sparse = True)\n",
        "        # TODO: define the linear forward layer\n",
        "        self.linear = nn.Linear(dim_embed, num_class)\n",
        "        # TODO: Initialize weights\n",
        "        self.init_weights()\n",
        "\n",
        "    # TODO: Define a method to initialize weights.\n",
        "    def init_weights(self):\n",
        "        # The weights should be random in the range of -0.5 to 0.5.\n",
        "        # You can initialize bias values as zero.\n",
        "        self.embedding.weight.data.uniform_(-0.5, 0.5)\n",
        "        self.linear.weight.data.uniform_(-0.5, 0.5)\n",
        "        self.linear.bias.data.zero_()\n",
        "    \n",
        "    # TODO: Define the forward function.\n",
        "    def forward(self, text, offsets):\n",
        "        # This should calculate the embeddings and return the linear layer\n",
        "        embeddings = self.embedding(text, offsets)\n",
        "        # with calculated embedding values.\n",
        "        return self.linear(embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R_67IxH9Ydy",
        "colab_type": "text"
      },
      "source": [
        "## Check your data before you proceed!\n",
        "\n",
        "Okay, so we know that we are using the `AG_NEWS` dataset in this project, but do you know what does the data contain? What is the format of the data? How many classes of data are there in this dataset? We do not know, yet. Let's find out!\n",
        "\n",
        "\n",
        "## Question 1:\n",
        "Create a new cell in this notebook and try to analyze the dataset that we loaded for you before. Report the following:\n",
        "* Vocabulary size (VOCAB_SIZE)\n",
        "* Number of classes (NUM_CLASS)\n",
        "* Names of the classes\n",
        "\n",
        "\n",
        "## Answer 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRnkmR_h14rh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
        "NUM_CLASS = len(train_dataset.get_labels())\n",
        "NAME_CLASSES = {\"World\",\"Sports\", \"Business\",\"Sci/Tec\"} #???"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZhiKih_4H9Z",
        "colab_type": "code",
        "outputId": "be9bcc7f-c4c7-4067-ce8a-f17b40fd57df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(VOCAB_SIZE)\n",
        "print(NUM_CLASS)\n",
        "print(NAME_CLASSES)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1308844\n",
            "4\n",
            "{'Sports', 'Sci/Tec', 'World', 'Business'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4ON6EBk7z3r",
        "colab_type": "text"
      },
      "source": [
        "Vocabulary size (VOCAB_SIZE) = 1308844\n",
        "\n",
        "Number of classes (NUM_CLASS) = 4\n",
        "\n",
        "Names of the classes = {'Sports', 'World', 'Business', 'Sci/Tec'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZdZ7hOv9Ydz",
        "colab_type": "text"
      },
      "source": [
        "## Create an instance for your model\n",
        "\n",
        "Great! You have successfully completed a basic analysis of the data that you are going to work with. The vocab size is equal to the length of vocab (including single word and ngrams). The number of classes is equal to the number of labels. Copy paste the code statements you used in your analysis to complete the code below. Also, using these parameters, create an instance `model` of your text classifier `TextClassifier`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2jOu6u99Yd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Paramters and model instance creation.\n",
        "'''\n",
        "\n",
        "# TODO: Instantiate the Vocabulary size and the number of classes\n",
        "# from the training dataset that we loaded for you.\n",
        "\n",
        "# Hint: Remember that these are PyTorch datasets. So, there should be \n",
        "# readily available functions that you can use to save time. ;)\n",
        "\n",
        "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
        "EMBED_DIM = 32\n",
        "NUM_CLASS = len(train_dataset.get_labels())\n",
        "\n",
        "# TODO: Instantiate the model with the parameters you defined above. \n",
        "# Remember to allocate it to your 'device' variable.\n",
        "\n",
        "model = text_classifier(VOCAB_SIZE, EMBED_DIM, NUM_CLASS).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0J8AsoT9Yd3",
        "colab_type": "text"
      },
      "source": [
        "## Generate batch\n",
        "\n",
        "Since the text entries have different lengths, you need to create a custom function to generate data batches and offsets. This function should be passed to the ``collate_fn`` parameter in the ``DataLoader`` call of pyTorch which you will use to create the data later on. The input to ``collate_fn`` is a list of tensors with the size of batch_size, and the ``collate_fn`` function packs them into a mini-batch. Pay attention here and make sure that ``collate_fn`` is declared as a top level definition. This ensures that the function is available in each worker. This is the reason why you need to define this custom function first before you call DataLoader().\n",
        "\n",
        "The text entries in the original data batch input are packed into a list and concatenated as a single tensor as the input of ``EmbeddingBag``. The offsets is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor. Label is a tensor saving the labels of individual text entries.\n",
        "\n",
        "Finish the function definition below. The function should take batch as an input parameter. Each entry in the batch contains a pair of values of the text and the corresponding label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XIyQjWf9Yd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Finish the function definition.\n",
        "\n",
        "def generate_batch(batch):\n",
        "    \n",
        "    label = torch.tensor([entry[0] for entry in batch])\n",
        "    text = [entry[1] for entry in batch]\n",
        "    offsets = [0] + [len(entry) for entry in text] \n",
        "\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0) \n",
        "    text = torch.cat(text)   #tensor text\n",
        "    \n",
        "    return text, offsets, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0rY3O379Yd6",
        "colab_type": "text"
      },
      "source": [
        "## Define the train function\n",
        "\n",
        "Here, you need to define a function which you will use later on in the project to train your model. This is very similar to the training steps that you have encountered before in previous coding assignment(s). The outline of the function is something like this -\n",
        "\n",
        "* load the data as batches\n",
        "* iterate over the batches\n",
        "* find the model output for a forward pass\n",
        "* calculate the loss\n",
        "* perform backpropagation on the loss (optimize it)\n",
        "* find the training accuracy\n",
        "\n",
        "In addition to this, you also need to find the total loss and total training accuracy values. Also, you need to return the average values of the total loss and total accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAJsUuPW9Yd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train(train_data):\n",
        "\n",
        "    # Initial values of training loss and training accuracy\n",
        "    \n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    total_acc = 0\n",
        "\n",
        "    # TODO: Use the PyTorch DataLoader class to load the data \n",
        "    # into shuffled batches of appropriate sizes into the variable 'data'.\n",
        "    # Remember, this is the place where you need to generate batches.\n",
        "    data = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle = True, collate_fn = generate_batch)\n",
        "    \n",
        "    \n",
        "    for i, (text, offsets, cls) in enumerate(data):\n",
        "        \n",
        "        # TODO: What do you need to do in order to perform backprop on the optimizer?\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        \n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "        \n",
        "        # TODO: Store the output of the model in variable 'output'\n",
        "        output = model(text, offsets)\n",
        "        \n",
        "        # TODO: Define the 'loss' variable (with respect to 'output' and 'cls').\n",
        "        # Also calculate the total loss in variable 'train_loss'\n",
        "        loss = criterion(output, cls)\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "        # TODO: Perform the backward propagation on 'loss' and \n",
        "        # optimize it through the 'optimizer' step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # TODO: Calculate and store the total training accuracy\n",
        "        # in the variable 'total_acc'.\n",
        "        # Remember, you need to find the \n",
        "        train_acc = (output.argmax(1) == cls).sum()\n",
        "        total_acc += train_acc.item()\n",
        "\n",
        "    # TODO: Adjust the learning rate here using the scheduler step\n",
        "    scheduler.step()\n",
        "    \n",
        "\n",
        "    return train_loss / len(train_data), total_acc / len(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YTFE7wu9Yd9",
        "colab_type": "text"
      },
      "source": [
        "## Define the test function\n",
        "\n",
        "Using the framework of the `train()` function in the previous cell, try to figure out the structure of the test function below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13I5CHsf9Yd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(test_data):\n",
        "    \n",
        "    # Initial values of test loss and test accuracy\n",
        "    \n",
        "    loss = 0\n",
        "    acc = 0\n",
        "    \n",
        "    # TODO: Use DataLoader class to load the data\n",
        "    # into non-shuffled batches of appropriate sizes.\n",
        "    # Remember, you need to generate batches here too.\n",
        "    data = DataLoader(test_data, batch_size = BATCH_SIZE, collate_fn = generate_batch)\n",
        "    \n",
        "    \n",
        "    for text, offsets, cls in data:\n",
        "        \n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "        \n",
        "        # Hint: There is a 'hidden hint' here. Let's see if you can find it :)\n",
        "        with torch.no_grad():  #why in this model we use no grad?\n",
        "            \n",
        "            # TODO: Get the model output\n",
        "            output = model(text, offsets)\n",
        "            \n",
        "            \n",
        "            # TODO: Calculate and add the loss to find total 'loss'\n",
        "            ls = criterion(output,cls)\n",
        "            loss += ls.item()\n",
        "            \n",
        "            \n",
        "            # TODO: Calculate the accuracy and store it in the 'acc' variable\n",
        "            a = (output.argmax(1) == cls).sum()\n",
        "            acc += a.item()\n",
        "            \n",
        "\n",
        "    return loss / len(test_data), acc / len(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBAjFk-Z9YeB",
        "colab_type": "text"
      },
      "source": [
        "## Split the dataset and run the model\n",
        "\n",
        "The original `AG_NEWS` has no validation dataset. For this reason, you need to split the training dataset into training and validation sets with a proper split ratio. The `random_split()` function in the torch.utils core PyTorch library should be able to help you with this. We have already imported it for you. :)\n",
        "\n",
        "* Consider the initial learning rate as 4.0, number of epochs as 5, training data ratio as 0.9.\n",
        "* You need to define and use a proper loss function\n",
        "* Define an Optimization algorithm (Suggestion: SGD)\n",
        "* Define a scheduler function to adjust the learning rate through epochs (gamma parameter = 0.9).\n",
        "(Hint: Look at the `StepLR` function)\n",
        "* Monitor the loss and accuracy values for both training and validation data sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU0fQ3S59YeB",
        "colab_type": "code",
        "outputId": "1adb710a-9e1c-4032-d0b1-0df9cd50a07c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "import time\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "# TODO: Set the number of epochs and the learning rate to \n",
        "# their initial values here\n",
        "\n",
        "N_EPOCHS = 5\n",
        "LEARNING_RATE = 4.0\n",
        "TRAIN_RATIO = 0.9\n",
        "\n",
        "# TODO: Set the intial validation loss to positive infinity\n",
        "init_valid_loss = float('inf')\n",
        "\n",
        "\n",
        "# TODO: Use the appropriate loss function\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "\n",
        "# TODO: Use the appropriate optimization algorithm with parameters (Suggested: SGD)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "\n",
        "# TODO: Use a scheduler function\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
        "\n",
        "\n",
        "# TODO: Split the data into train and validation sets using random_split()\n",
        "len_trainset = int(len(train_dataset)*0.95)\n",
        "len_validset = len(train_dataset) - len_trainset\n",
        "train_set , valid_set = random_split(train_dataset, [len_trainset, len_validset])\n",
        "\n",
        "\n",
        "# TODO: Finish the rest of the code below\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train(train_set)\n",
        "    valid_loss, valid_acc = test(valid_set)\n",
        "\n",
        "    secs = int(time.time() - start_time)\n",
        "    mins = secs / 60\n",
        "    secs = secs % 60\n",
        "\n",
        "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  | time in 0 minutes, 9 seconds\n",
            "\tLoss: 0.0261(train)\t|\tAcc: 84.8%(train)\n",
            "\tLoss: 0.0195(valid)\t|\tAcc: 89.3%(valid)\n",
            "Epoch: 2  | time in 0 minutes, 8 seconds\n",
            "\tLoss: 0.0119(train)\t|\tAcc: 93.6%(train)\n",
            "\tLoss: 0.0178(valid)\t|\tAcc: 90.8%(valid)\n",
            "Epoch: 3  | time in 0 minutes, 9 seconds\n",
            "\tLoss: 0.0069(train)\t|\tAcc: 96.3%(train)\n",
            "\tLoss: 0.0199(valid)\t|\tAcc: 90.1%(valid)\n",
            "Epoch: 4  | time in 0 minutes, 9 seconds\n",
            "\tLoss: 0.0038(train)\t|\tAcc: 98.1%(train)\n",
            "\tLoss: 0.0200(valid)\t|\tAcc: 91.1%(valid)\n",
            "Epoch: 5  | time in 0 minutes, 9 seconds\n",
            "\tLoss: 0.0022(train)\t|\tAcc: 99.0%(train)\n",
            "\tLoss: 0.0211(valid)\t|\tAcc: 91.1%(valid)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1m5w_ee9YeE",
        "colab_type": "text"
      },
      "source": [
        "## Let's  check the test loss and test accuracy\n",
        "\n",
        "So you have trained your model and seen how well it performs on the training and validation datasets. Now, you need to check your model's performance against the test dataset. Using the test dataset as input, report the test loss and test accuracy scores of your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K5hYgD-9YeF",
        "colab_type": "code",
        "outputId": "4ec75760-b51c-4987-e190-60f588a9ebf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# TODO: Compete the code below to find \n",
        "# the results (loss and accuracy) on the test data\n",
        "\n",
        "print('Checking the results of test dataset...')\n",
        "test_loss, test_acc = test(test_dataset)\n",
        "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking the results of test dataset...\n",
            "\tLoss: 0.0236(test)\t|\tAcc: 90.6%(test)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3-11uuP9YeH",
        "colab_type": "code",
        "outputId": "2c5f319b-93df-413d-fc56-696e5dfde6cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# importing necessary libraries\n",
        "\n",
        "import re\n",
        "from torchtext.data.utils import ngrams_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# labels for the AG_NEWS dataset\n",
        "\n",
        "ag_news_label = {1 : \"World\",\n",
        "                 2 : \"Sports\",\n",
        "                 3 : \"Business\",\n",
        "                 4 : \"Sci/Tec\"}\n",
        "\n",
        "def predict(text, model, vocab, ngrams):\n",
        "    tokenizer = get_tokenizer(\"basic_english\")\n",
        "    with torch.no_grad():\n",
        "      #vocab[token] : tokens_ids\n",
        "        text = torch.tensor([vocab[token]\n",
        "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
        "        output = model(text, torch.tensor([0]))\n",
        "        return output.argmax(1).item() + 1\n",
        "\n",
        "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
        "    enduring the season’s worst weather conditions on Sunday at The \\\n",
        "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
        "    considering the wind and the rain was a respectable showing. \\\n",
        "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
        "    was another story. With temperatures in the mid-80s and hardly any \\\n",
        "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
        "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
        "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
        "    was even more impressive considering he’d never played the \\\n",
        "    front nine at TPC Southwind.\"\n",
        "\n",
        "vocab = train_dataset.get_vocab()\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "# TODO: Predict the topic of the above given random text (use bigrams)\n",
        "# Use the proper paramters in the predict() function\n",
        "\n",
        "print(\"This is a '%s' news\" % ag_news_label[predict(ex_text_str, model, vocab, 2)])\n",
        "\n",
        "# If you have done everything correctly in this task,\n",
        "# then the output of this cell should be - \"This is a 'Sports' news\"."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a 'Sports' news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVZBI0NF9YeK",
        "colab_type": "text"
      },
      "source": [
        "# Congratulations! You just designed your first neural classifier!\n",
        "\n",
        "And probably you have achieved a good accuracy score too. Great job!\n",
        "\n",
        "## Question 2:\n",
        "You just tested your model with a new sample text. Try to feed some more random examples of similar text (which you think are related to at least one of the four topics _\"World\", \"Sports\", \"Business\", \"Sci/Tec\"_ of our problem) to the model and see how your model reacts. Give at least 3 such examples (You are free to include more examples if you wish to).\n",
        "\n",
        "## Answer 2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZlzcf8I3Q8P",
        "colab_type": "code",
        "outputId": "e57fd08f-a55e-4ea7-9c3d-f9741ccc32ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#first example\n",
        "\n",
        "ex_text_str = \"Two of Australia's bush fires are likely to merge into a so-called 'mega blaze' on Friday evening, authorities have warned.\\\n",
        "The merger is expected at the border of New South Wales and Victoria and has been feared for days.\\\n",
        "Prime Minister Scott Morrison warned Friday would be 'a difficult day in the eastern states' amid forecasts of heat, strong winds and dry lightning.\\\n",
        "In South Australia, Kangaroo Island also faced an abrupt threat.\\\n",
        "A spokesman for the New South Wales (NSW) Rural Fire Service told the BBC the merger of two fires - both of which are out of control - was 'imminent' and expected at about 8pm (09:00 GMT).\\\n",
        "The two fires at Dunns Road and East Ournie Creek has firefighters bracing for a difficult night - and aircraft won't be able to operate after dark.\\\n",
        "More than 100 bushfires are burning in worst-hit NSW alone, but the danger is equally great in Victoria.\\\n",
        "Victoria's Country Fire Authority issued several emergency warnings on Friday, telling people to evacuate before it became too dangerous.\\\n",
        "In parts of both Victoria and NSW, authorities urged people to leave their homes 'to avoid tragedy'.\\\n",
        "Fires in NSW have destroyed about 1,000 homes since the New Year.\\\n",
        "Mr Morrison said that two ships remained off the coast of NSW ready to evacuate towns if needed.\"\n",
        "\n",
        "vocab = train_dataset.get_vocab()\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "# TODO: Predict the topic of the above given random text (use bigrams)\n",
        "# Use the proper paramters in the predict() function\n",
        "\n",
        "print(\"This is a '%s' news\" % ag_news_label[predict(ex_text_str, model, vocab, 2)])\n",
        "\n",
        "# then the output of this cell should be - \"This is a 'World' news\"."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a 'World' news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzOmjbLy6UQT",
        "colab_type": "code",
        "outputId": "b68e26e6-f2f3-4e0b-efce-6af16c79ac12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Second example\n",
        "\n",
        "ex_text_str = \"Demand for iPhones appears to be flourishing once again in China, a year after Apple had to warn investors that the Chinese market was facing a serious slow down.\\\n",
        "IPhone sales in China were up 18% in December from the same month a year earlier, an even better performance than Wall Street had projected, according to an investor note from Wedbush analyst Dan Ives. Apple (AAPL) shipped around 3.2 million iPhones to China during the month compared to 2.7 million in December 2018, Ives reported, citing data from the China Academy of Information and Communication Technology.\\\n",
        "It's good news for Apple, after iPhone sales tumbled in China over the past year.\\\n",
        "'Our belief that China will continue this positive upward trajectory with renewed growth and share gains on the heels of an iPhone 11 product cycle which the skeptics continue to underestimate,' Ives said in the Thursday note.\\\n",
        "The good news was reflected in Apple's stock, which was up nearly 2% to a record high on Thursday.\\\n",
        "China is a key market for Apple — the region makes up nearly 17% of the company's total sales. And the iPhone is Apple's biggest profit driver.\\\n",
        "In early January 2019, Apple CEO Tim Cook wrote a letter to investors warning them to expect lower sales from the holiday quarter due primarily to iPhone sales in China falling short of what the company had expected. It was the first time since June 2002 that Apple issued a reduction in its quarterly revenue forecast. When the company reported earnings for that quarter later in January, iPhone sales had fallen 15% from the prior year.\\\n",
        "A number of factors contributed to the drop, namely slower growth in the Chinese economy and the US-China trade war.\\\n",
        "The trend continued throughout much of last year.\\\n",
        "In April, Apple said its iPhone sales in the first three months of 2019 dropped 17% from the same period a year earlier, again because of sluggish demand in China. A month later, Citi analysts warned that the trade war could cause Apple's iPhone sales in China to be cut in half.\\\n",
        "In the three months ending in June 2019, iPhones made up less than half of the company's revenue for the first time in years, though the slump also coincided with a greater focus at Apple on subscription-based services such as Apple Music.\\\n",
        "But the iPhone 11, which Apple introduced in September with better camera technology and battery life, as well as lower-than-expected prices, has helped with the rebound, Ives said. Early demand for the new model was strong, and in Apple's October earnings call, Cook noted that the company's prospects in China were turning around.\\\n",
        "Now, Ives estimates that there are roughly 60 million to 70 million iPhone users in China who are likely to upgrade their phones in the coming months.\\\n",
        "The momentum probably will continue this year, as Apple analysts widely expect Apple to release a 5G-enabled version of the iPhone in the fall.\\\n",
        "'Many investors are asking us: Is all the good news baked into shares after an historic upward move over the last year?' Ives said in the note. 'The answer from our vantage point is a resounding NO, as we view [this as] only the first part of this massive upgrade opportunity.\"\n",
        "\n",
        "vocab = train_dataset.get_vocab()\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "# TODO: Predict the topic of the above given random text (use bigrams)\n",
        "# Use the proper paramters in the predict() function\n",
        "\n",
        "print(\"This is a '%s' news\" % ag_news_label[predict(ex_text_str, model, vocab, 2)])\n",
        "\n",
        "# then the output of this cell should be - \"This is a 'Bussiness' news\". But it reports Sci/Tec, because the text is selected from CNN bussiness news"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a 'Business' news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72AWvHw-6Oa0",
        "colab_type": "code",
        "outputId": "6ab24908-164e-4fb7-8c8c-562c81996bda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Third example\n",
        "\n",
        "ex_text_str = \"Amazon, Alphabet, Alibaba, Facebook, Tencent - five of the world's 10 most valuable companies,\\\n",
        " all less than 25 years old - and all got rich, in their own ways, on data.No wonder it's become common to call data the 'new oil'.\\\n",
        "  As recently as 2011, five of the top 10 were oil companies. Now, only ExxonMobil clings on.\\\n",
        "The analogy isn't perfect. Data can be used many times, oil only once.\\\n",
        "But data is like oil in that the crude, unrefined stuff is not much use to anyone.\\\n",
        "You have to process it to get something valuable. You refine oil to make diesel, to put it in an engine.\\\n",
        "With data, you need to analyse it to provide insights that can inform decisions - which advert to insert in a social media timeline, which search result to put at the top of the page.\\\n",
        "Imagine you were asked to make just one of those decisions.\\\n",
        "Someone is watching a video on YouTube, which is run by Google, which is owned by Alphabet. What should the system suggest they watch next?\\\n",
        " Pique their interest, and YouTube gets to serve them another advert. Lose their attention, and they will click away.\\\n",
        "You have all the data you need. Consider every other YouTube video they have ever watched - what are they interested in? Now, look at what other users have gone on to watch after this video.\\\n",
        "Weigh up the options, calculate probabilities. If you choose wisely, and they view another ad, well done - you've earned Alphabet all of, ooh, maybe 20 cents (15p).\\\n",
        "Clearly, relying on humans to process data would be impossibly inefficient. These business models need machines.\\\n",
        "In the data economy, power comes not from data alone but from the interplay of data and algorithm.\"\n",
        "\n",
        "vocab = train_dataset.get_vocab()\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "# TODO: Predict the topic of the above given random text (use bigrams)\n",
        "# Use the proper paramters in the predict() function\n",
        "\n",
        "print(\"This is a '%s' news\" % ag_news_label[predict(ex_text_str, model, vocab, 2)])\n",
        "\n",
        "# then the output of this cell should be - \"This is a 'Sci\\Tec' news\". But it reports Bussiness, because the text is selected from BBC news"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a 'Business' news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACZsrygMCrq9",
        "colab_type": "code",
        "outputId": "7fcaf27f-60d2-42d9-be87-18b1eea5cd47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Fourth example\n",
        "\n",
        "ex_text_str = \"Nreal, the Chinese start-up involved, has confounded the expectations of many industry watchers with the quality of the images its Light glasses produces.\\\n",
        "The firm still faces issues.\\\n",
        "One tester said the glasses looked a bit 'clunky', and the company is being sued by Magic Leap, a rival.\\\n",
        "But long-time CES attendee Ben Wood, an influential tech consultant, declared them the 'product of the show'.\\\n",
        "For years people have over-promised and under-delivered on augmented reality glasses,' the CCS Insight analyst told the BBC.\\\n",
        "'Nreal seem to have quietly got on with delivering the product and are now set to ship it by the middle of the year.\\\n",
        "'I'm not gong to pretend the glasses will be to everybody's taste - this is still a first-generation product. But they are a lot closer to a normal pair of sunglasses than some of the other bulky smart glasses I've seen, and they definitely provide the best experience of augmented reality glasses at CES.'\"\n",
        "\n",
        "vocab = train_dataset.get_vocab()\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "# TODO: Predict the topic of the above given random text (use bigrams)\n",
        "# Use the proper paramters in the predict() function\n",
        "\n",
        "print(\"This is a '%s' news\" % ag_news_label[predict(ex_text_str, model, vocab, 2)])\n",
        "\n",
        "# then the output of this cell should be - \"This is a 'Sci\\Tec' news\"."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a 'Sci/Tec' news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD3j7GaI9YeK",
        "colab_type": "text"
      },
      "source": [
        "## Question 3:\n",
        "Okay, probably the model still works great with the examples you fed to it in the previous question. How about a twist in the plot? Let's feed it some more random text data from completely different genres/topics (not belonging to the 4 topics which we talk about the in the first question). How does your model react now? Give at least 3 such examples (You are free to include more examples if you wish to).\n",
        "\n",
        "Of course the predictions will be limited to the four class labels that your model is trained on. Can you somehow justify the labels that your model predicted now for the given text inputs?\n",
        "\n",
        "## Answer 3:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1n5AgnyInsE",
        "colab_type": "code",
        "outputId": "4f72ec09-681c-4c21-dd15-90c7c155ffc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#example 1:\n",
        "\n",
        "ex_text_str = \"More than 50 people have been infected. Seven are currently in a critical condition.\\\n",
        "A new virus arriving on the scene, leaving patients with pneumonia, is always a worry and health officials around the world are on high alert.\\\n",
        "But is this a brief here-today-gone-tomorrow outbreak or the first sign of something far more dangerous?\\\n",
        "What is this virus?\\\n",
        "Viral samples have been taken from patients and analysed in the laboratory.\\\n",
        "And officials in China and the World Health Organization have concluded the infection is a coronavirus.\\\n",
        "Coronaviruses are a broad family of viruses, but only six (the new one would make it seven) are known to infect people.\\\n",
        "Severe acute respiratory syndrome (Sars), which is caused by a coronavirus, killed 774 of the 8,098 people infected in an outbreak that started in China in 2002.\\\n",
        "'There is a strong memory of Sars, that's where a lot of fear comes from, but we're a lot more prepared to deal with those types of diseases,' says Dr Josie Golding, from the Wellcome Trust.\\\n",
        "Where has it come from?\\\n",
        "New viruses are detected all the time.\\\n",
        "They jump from one species, where they went unnoticed, into humans.\\\n",
        "'If we think about outbreaks in the past, if it is a new coronavirus, it will have come from an animal reservoir,' says Prof Jonathan Ball, a virologist at the University of Nottingham.\\\n",
        "Sars jumped from the civet cat into humans.\\\n",
        "And Middle East respiratory syndrome (Mers), which has killed 858 out of the 2,494 recorded cases since it emerged in 2012, regularly makes the jump from the dromedary camel.\"\n",
        "\n",
        "vocab = train_dataset.get_vocab()\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "# TODO: Predict the topic of the above given random text (use bigrams)\n",
        "# Use the proper paramters in the predict() function\n",
        "\n",
        "print(\"This is a '%s' news\" % ag_news_label[predict(ex_text_str, model, vocab, 2)])\n",
        "\n",
        "#The output of this cell should be - \"This is a 'Health' news\"."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a 'Sci/Tec' news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4EoMe49IoZb",
        "colab_type": "code",
        "outputId": "0986b07e-b33c-43c9-dbbf-6794e39944ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#example 2:\n",
        "\n",
        "ex_text_str = \"Architecture can flirt with nature in expressive yet subtle ways. The idea is, often, to harmonize, not dominate, the landscapes.\\\n",
        "This can prove a challenge, however, when faced with steep slopes, cliff faces and mountainsides.\\\n",
        "Some of today's most interesting architects are out to prove the discipline can be edgy -- quite literally. Here is one examples of houses that overcome difficult environments to offer extraordinary an experience for owners and onlookers alike.\\\n",
        "Cliff House, on the Atlantic coast in Nova Scotia, is an inventive and playful intervention in the landscape.\\\n",
        "From hill height, the house looks absolutely normal. But from the\\ coast, you can see it's actually perched on a cliff, which the architects say is intended 'to heighten one's experience of the landscape through a sense of vertigo and a sense of floating on the sea.'\\\n",
        "The galvanized steel superstructure provides solid support and is fixed to the cliff, while wooden elements introduce cosiness inside and out.\\\n",
        "The cube is not divided into levels, so the large living space fills the entire area. Only a small part of it is transformed into sleeping quarters.\"\n",
        "\n",
        "vocab = train_dataset.get_vocab()\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "# TODO: Predict the topic of the above given random text (use bigrams)\n",
        "# Use the proper paramters in the predict() function\n",
        "\n",
        "print(\"This is a '%s' news\" % ag_news_label[predict(ex_text_str, model, vocab, 2)])\n",
        "\n",
        "#The output of this cell should be - \"This is a 'Architecture / Style/ Art' news\"."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a 'Business' news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7ZdaUn4IolO",
        "colab_type": "code",
        "outputId": "ce710c0b-685c-4654-a4fe-afa5e26760c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#example 3:\n",
        "\n",
        "ex_text_str = \"Cambodian cuisine has a long history and a diverse range of influences, yet it's only now becoming known beyond the country's borders. In fact, the only place you can experience all it has to offer is in the country itself. \\\n",
        "Here is one of the 30 best dishes to try.\\\n",
        "Samlor korkor\\\n",
        "While amok is sometimes called the country's national dish, and might be the one most familiar to tourists, samlor korkor has a better claim to being the true national dish of Cambodia. It has been eaten for hundreds of years and today can be found in restaurants, roadside stands and family homes alike.\\\n",
        "The ingredients list for this nourishing soup is versatile and easily adapted to whatever is seasonal and abundant; it often includes more than a dozen vegetables. It can be made with almost any type of meat, but most commonly it's a hearty soup made from catfish and pork belly. The soup always includes two quintessential Cambodian ingredients -- prahok, a type of fermented fish, and kroeung, a fragrant curry paste -- and is then thickened with toasted ground rice.\"\n",
        "\n",
        "vocab = train_dataset.get_vocab()\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "# TODO: Predict the topic of the above given random text (use bigrams)\n",
        "# Use the proper paramters in the predict() function\n",
        "\n",
        "print(\"This is a '%s' news\" % ag_news_label[predict(ex_text_str, model, vocab, 2)])\n",
        "\n",
        "#The output of this cell should be - \"This is a 'Travel' news\"."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a 'Business' news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4IVfTwV9YeL",
        "colab_type": "text"
      },
      "source": [
        "## Question 4:\n",
        "Your model probably has achieved a good accuracy score. However, there may be lots of things that you could still try to do to improve your classifier model. Can you try to list down some improvements that you think would be able to improve the above model's performance?\n",
        "\n",
        "_(Hint: Maybe think about alternate architectures, #layers, hyper-paramters, etc..., but try not to come up with too complex stuff! :) )_\n",
        "\n",
        "## Answer 4:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0wjtpYjjZ3S",
        "colab_type": "text"
      },
      "source": [
        "For improving our classifier model,\n",
        "\n",
        "1) we can use differenr Architecture:\n",
        "\n",
        "Convolutional Neural Networks for Sentence Classification (CNN)\n",
        "\n",
        "Recurrent Neural Networks (RNN)\n",
        "\n",
        "Gated Recurrent Unit (GRU)\n",
        "\n",
        "Long Short-Term Memory (LSTM)\n",
        "\n",
        "Hierarchical Attention Networks for Document Classification \n",
        "\n",
        "Hierarchical Deep Learning for Text (HDLTex)\n",
        "\n",
        "2) We can change hyper parameters in order to have beter results:\n",
        "value 4.0 for learning rate is so hight by considering 0.01 and Epoch = 50 we can have better accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fhOJYOlhBXr",
        "colab_type": "text"
      },
      "source": [
        "# Task 2: Try the better option that you proposed\n",
        "\n",
        "In Question 4, you have proposed some alternate solution that you think will be able to somehow improve your model. Following one of the options below, try to build and train a new model, and report the new loss and accuracy scores. Is it better than your initial classifier model for the same data?\n",
        "\n",
        "For your reference, here are some neural models using which researchers have tried to classify text before:\n",
        "\n",
        "* Recurrent Neural Networks (RNNs)\n",
        "* Long-Short Term Memory (LSTM)\n",
        "* Bi-directional LSTM (BiLSTM)\n",
        "* Gated Recurrent Units (GRUs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgAycqqq9YeM",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "# Task 3: Let your creativity flow!\n",
        "\n",
        "As discussed earlier, you are free to come up with anything in task 3. Think and try to model unique (not too complex!) neural architecture on your own. Remember that this model has to be novel as much as possible, so try not to copy other people's existing work. Using the same data, train the new model, and report the accuracy scores. How much better/worse is this model than the previous two models? Why do you think this is better/worse?\n",
        "\n",
        "# Important Notes\n",
        "\n",
        "## NOTE 1:\n",
        "If you want, you can try out the models on other datasets too for comparisons. Although this is not mandatory, it would be really interesting to see how your model performs for data from different domains maybe. Note that you may need to tweak the code a little bit when you are considering other datasets and formats. \n",
        "\n",
        "## NOTE 2:\n",
        "Any form of plagiarism is strictly prohibited. If it is found that you have copied sample code from the internet, the entire team will be penalized.\n",
        "\n",
        "## NOTE 3:\n",
        "Often Jupyter Notebooks tend to stop working or crash due to overload of memory (lot of variables, big neural models, memory-intensive training of models, etc...). Moreover, with more number of tasks, the number of variables that you will be using will surely incerase. Therefore, it is recommended that you use separate notebooks for each _Task_ in this project.\n",
        "\n",
        "## NOTE 4:\n",
        "You are expected to write well-documented code, that is, with proper comments wherever you think is needed. Make sure you write a comprehensive report for the entire project consisting of data analysis, your model architecture, methods used, discussing and comparing the models against the accuracy and loss metrics, and a final conslusion. If you want to prepare separate reports for each _Task_, you could do this in the Jupyter Notebook itself using $Mardown$ and $\\LaTeX$ code if needed. If you want to submit a single report for the entire project, you could submit a PDF file in that case (Word or $\\LaTeX$).\n",
        "\n",
        "All the very best for project 2. Wishing you happy holidays and a very happy new year in advance! :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKsNKWIu9YeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}